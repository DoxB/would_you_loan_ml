{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. 지역 전처리 함수 정의\n",
    "def process_location(location):\n",
    "    major_regions = {\n",
    "        '서울': ['서울', '중구', '여의도', '영등포', '강남', '서초', '종로', '마포', '강북', '강동', '송파', '은평', '동대문', '서대문', '성북', '용산'],\n",
    "        '대전': ['대전', '유성', '서구', '동구', '중구'],\n",
    "        '부산': ['부산', '해운대', '수영', '남구', '중구', '동구', '북구', '사하', '사상', '연제', '기장'],\n",
    "        '대구': ['대구', '중구', '동구', '서구', '남구', '북구', '수성', '달서', '달성'],\n",
    "        '충청도': ['충북', '충청북도', '충남', '충청남도', '청주', '천안', '공주', '논산', '제천', '아산', '서산'],\n",
    "        '경기도': ['경기', '수원', '용인', '성남', '고양', '안산', '부천', '화성', '남양주', '의정부', '평택', '광명', '군포', '하남', '파주'],\n",
    "        '경상도': ['경북', '경상북도', '경남', '경상남도', '창원', '김해', '양산', '거제', '진주', '포항', '경주', '울산'],\n",
    "        '전라도': ['전북', '전라북도', '전남', '전라남도', '전주', '익산', '군산', '여수', '목포', '순천', '광주'],\n",
    "        '강원도': ['강원', '강릉', '춘천', '원주', '속초'],\n",
    "        '제주도': ['제주', '서귀포']\n",
    "    }\n",
    "    \n",
    "    regions = [region.strip() for region in str(location).split(',') if region.strip()]\n",
    "    result = []\n",
    "    for region in regions:\n",
    "        for major, subregions in major_regions.items():\n",
    "            if any(subregion in region for subregion in subregions):\n",
    "                result.append(region if region in subregions else major)\n",
    "                break\n",
    "        else:\n",
    "            result.append('기타')\n",
    "    \n",
    "    return list(set(result))  # 중복 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  본문       위치 전처리된 위치  감성\n",
      "0  ■국세청 ◇서기관 승진 △국세청 혁신정책담당관실 이우진 △ 기획재정담당관실 박찬웅 ...  고광덕,김덕은    [기타]  긍정\n",
      "1                                             [사진기사]        中    [기타]  중립\n",
      "2  ◇산업통상자원부 \\n \\n△과장급 전보 \\n \\n▷정보관리담당관 김열규 ▷무역정책과...      NaN    [기타]  긍정\n",
      "3  [이투데이] 세종=조아라 기자 (abc@etoday.co.kr)\\n\\n◇과장급 전보...       세종    [기타]  긍정\n",
      "4  ◇국세청 △서기관 승진 ▷혁신정책담당관실 이우진 ▷기획재정담당관실 박찬웅 ▷감찰담당...      NaN    [기타]  긍정\n"
     ]
    }
   ],
   "source": [
    "# 2. 감성 예측 모델 정의 및 초기화\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "bert_model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert, hidden_size=768, num_classes=3, dr_rate=0.5):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n",
    "        out = self.dropout(pooled_output)\n",
    "        return self.classifier(out)\n",
    "\n",
    "# 감성 예측 모델 초기화\n",
    "model = BERTClassifier(bert_model, num_classes=3)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 감성 예측 함수 정의\n",
    "def predict_sentiment(text):\n",
    "    inputs = tokenizer(text, max_length=64, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        pred_label = torch.argmax(probs, dim=1).cpu().numpy()[0]\n",
    "    \n",
    "    sentiment_map = {0: '부정', 1: '중립', 2: '긍정'}\n",
    "    return sentiment_map[pred_label]\n",
    "\n",
    "# 3. CSV 파일을 로드하고 각 기사에 대해 분석 수행\n",
    "df = pd.read_csv('./NewsResult.csv')\n",
    "\n",
    "# 결측값을 빈 문자열로 대체하여 문자열 결합 가능하게 처리\n",
    "df['본문'] = df['본문'].fillna(\"\").astype(str)\n",
    "df['키워드'] = df['키워드'].fillna(\"\").astype(str)\n",
    "\n",
    "# 결과 저장을 위한 리스트 생성\n",
    "processed_locations = []\n",
    "sentiments = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    location = row['위치']\n",
    "    text = row['본문'] + \" \" + row['키워드']\n",
    "\n",
    "    # 위치 전처리 및 감성 예측\n",
    "    processed_locations.append(process_location(location))\n",
    "    sentiments.append(predict_sentiment(text))\n",
    "\n",
    "# 전처리 및 예측 결과를 데이터프레임에 추가\n",
    "df['전처리된 위치'] = processed_locations\n",
    "df['감성'] = sentiments\n",
    "\n",
    "# 결과 확인\n",
    "print(df[['본문', '위치', '전처리된 위치', '감성']].head())\n",
    "\n",
    "# 필요 시, 결과 저장\n",
    "df.to_csv('Processed_NewsResult.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
